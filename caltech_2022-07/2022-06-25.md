```
#Type the following command on the master node:
sudo hostnamectl set-hostname master.example.com
exec bash
```
```
#Type the following command on worker 1:
sudo hostnamectl set-hostname worker-node-1.example.com
exec bash
```
```
#Type the following command on worker 2:
sudo hostnamectl set-hostname worker-node-2.example.com
exec bash
```
```
#Note: Run the following command on the master and ALL nodes:
sudo kubeadm reset --force
sudo mkdir /etc/docker
sudo tee /etc/docker/daemon.json 0<<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
sudo systemctl enable docker
sudo systemctl daemon-reload
sudo systemctl restart docker
sudo swapoff -a
```
```
#Now you can run the following commands on the master node:
log=${HOME}/install-leader.log
#The following network address allows a maximum of approximately 60K IP addresses so that means 60K Pods
#There is a maximum of approximately 100 Pods per worker node so that means 60K/100 = 600 worker nodes
#There is a limit of approximately 5K worker nodes per Kubernetes cluster
#In case I wanted to reach that limit then I would need to increase the pod_network_cidr parameter
#For example: pod_network_cidr=10.0.0.0/8 that would allow approximately 10M Pods so that means 10M/100 = 100K worker nodes
#Obviously before reaching the limit of 100K worker nodes we would first reach the limit of 5K worker nodes
pod_network_cidr=192.168.0.0/16
sudo kubeadm init --pod-network-cidr ${pod_network_cidr} --ignore-preflight-errors all 2>&1 | tee ${log}
```
```
#Run the following commands on the master node to allow non-root users to access use kubectl:
mkdir -p ${HOME}/.kube
sudo cp /etc/kubernetes/admin.conf ${HOME}/.kube/config
sudo chown -R $( id -u ):$( id -g ) ${HOME}/.kube/
echo 'source <(kubectl completion bash)' | tee --append ${HOME}/.bashrc
source ${HOME}/.bashrc
```
```
#Type the following command to add the Weave Net CNI plugin for highly available clusterUse:
kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')
```
```
#Run the following command for joining the worker nodes to the cluster and save it:
sudo kubeadm token create --print-join-command
```
```
#Go to Worker1 node and run the kubeadm join command copied in the previous step to join this node as a worker node to the cluster:
#REMEMBER TO ADD SUDO AT THE BEGINNING OF THE COMMAND
sudo kubeadm join x.x.x.x:6443 --token xxx.xxx --discovery-token-ca-cert-hash sha256:xxxxxx 
```
```
#Go to the Worker2 node and repeat the previous step to join this node as a worker node to the cluster
```
```
#Navigate to the master node tab and verify the nodes that are added to the cluster
#Run the following commands:
kubectl get no
kubectl get ns
kubectl get po -A
```
Experiment to demonstrate Kubernetes networking with IPtables:
```
kubectl apply -f https://raw.githubusercontent.com/sebastian-colomar/phpinfo/main/kube-compose-po-echo.yaml
kubectl get all
```
Wait until the deployment is stable and then continue with the following commands:
```
#Retrieve the Node Port for the deployed service:
nodePort=$( kubectl get svc/phpinfo-po-echo | awk -F : /TCP/'{ print $2 }' | cut -d / -f 1 )
#Retrieve the CHAIN for the Service associated with that Node Port:
kube_svc=$( sudo iptables -S -t nat | grep A.KUBE-NODEPORTS.*${nodePort} | awk '{ print $14 }' )
#Retrieve the CHAIN for the Target associated to that Service:
kube_sep=$( sudo iptables -S -t nat | grep A.${kube_svc}.*KUBE-SEP | awk '{ print $8 }' )
#Retrieve the IP address and Port of the target:
destination=$( sudo iptables -S -t nat | grep A.${kube_sep}.*DNAT | awk '{ print $14 }' )
```
To view the IP tables rules:
```
kubectl get svc/phpinfo-po-echo
sudo iptables -S -t nat | grep A.KUBE-NODEPORTS.*${nodePort}
sudo iptables -S -t nat | grep A.${kube_svc}.*KUBE-SEP
sudo iptables -S -t nat | grep A.${kube_sep}.*DNAT
```
Let us delete the deployment and check that the IP tables rules have been immediately removed:
```
kubectl delete -f https://raw.githubusercontent.com/sebastian-colomar/phpinfo/main/kube-compose-po-echo.yaml
kubectl get svc/phpinfo-po-echo
sudo iptables -S -t nat | grep A.KUBE-NODEPORTS.*${nodePort}
sudo iptables -S -t nat | grep A.${kube_svc}.*KUBE-SEP
sudo iptables -S -t nat | grep A.${kube_sep}.*DNAT
```
Now redeploy:
```
kubectl apply -f https://raw.githubusercontent.com/sebastian-colomar/phpinfo/main/kube-compose-po-echo.yaml
kubectl get all
```
Wait until the deployment is stable again and then continue with the following commands:
```
#Retrieve the Node Port for the deployed service:
nodePort=$( kubectl get svc/phpinfo-po-echo | awk -F : /TCP/'{ print $2 }' | cut -d / -f 1 )
#Retrieve the CHAIN for the Service associated with that Node Port:
kube_svc=$( sudo iptables -S -t nat | grep A.KUBE-NODEPORTS.*${nodePort} | awk '{ print $14 }' )
#Retrieve the CHAIN for the Target associated to that Service:
kube_sep=$( sudo iptables -S -t nat | grep A.${kube_svc}.*KUBE-SEP | awk '{ print $8 }' )
#Retrieve the IP address and Port of the target:
destination=$( sudo iptables -S -t nat | grep A.${kube_sep}.*DNAT | awk '{ print $14 }' )
```
To view the IP tables rules:
```
kubectl get svc/phpinfo-po-echo
sudo iptables -S -t nat | grep A.KUBE-NODEPORTS.*${nodePort}
sudo iptables -S -t nat | grep A.${kube_svc}.*KUBE-SEP
sudo iptables -S -t nat | grep A.${kube_sep}.*DNAT
```
In order to locate the Pod:
```
kubectl describe po | grep Node:
kubectl exec phpinfo-po-echo -c phpinfo-container -- cat index.php
kubectl exec phpinfo-po-echo -c phpinfo-container -- ps aux
```
