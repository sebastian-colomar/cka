```
kubectl config get-clusters|grep -v NAME|tee /opt/course/1/contexts

echo kubectl config current-context|tee /opt/course/1/context_default_kubectl.sh

echo 'grep current .kube/config|cut -d\  -f2'|tee /opt/course/1/context_default_no_kubectl.sh

kubectl run --dry-run=client -oyaml --image httpd:2.4.41-alpine -n default pod1|tee 02.yaml

kubectl get no --show-labels|grep control-plane

kubectl taint no cluster1-controlplane1 node-role.kubernetes.io/control-plane:NoSchedule-
```
```
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  namespace: default
spec:
  containers:
  - image: httpd:2.4.41-alpine
    name: pod1-container
  nodeSelector:
    node-role.kubernetes.io/control-plane: ''
  tolerations:
  - key: node-role.kubernetes.io/control-plane
    effect: NoSchedule
```
```
kubectl -n project-c13 get po o3db-0 -oyaml|grep ownerReferences: -A6

kubectl -n project-c13 scale --replicas 1 sts o3d

kubectl run -n default --dry-run=client -oyaml ready-if-service-ready --image nginx:1.16.1-alpine|tee 04.yaml

kubectl explain po.spec.containers.livenessProbe.exec
```
```
apiVersion: v1
kind: Pod
metadata:
  name: ready-if-service-ready
  namespace: default
spec:
  containers:
  - image: nginx:1.16.1-alpine
    name: ready-if-service-ready
    livenessProbe:
      exec:
        command:
        - "true"
    readinessProbe:
      exec:
        command:
        - "wget -T2 -O- http://service-am-i-ready:80"
```
```
kubectl events --for po/ready-if-service-ready

kubectl get ep

kubectl get svc

kubectl explain po.spec.containers.livenessProbe.exec.command

kubectl run --dry-run=client -oyaml -n default am-i-ready --image nginx:1.16.1-alpine -l id=cross-server-ready|tee 04b.yaml
```
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    id: cross-server-ready
  name: am-i-ready
  namespace: default
spec:
  containers:
  - image: nginx:1.16.1-alpine
    name: am-i-ready
```
```
kubectl get po -owide

kubectl exec ready-if-service-ready -- wget -T2 -O- http://service-am-i-ready:80 -Sq --spider

kubectl delete -f 04.yaml;kubectl create -f 04.yaml
```
```
echo 'kubectl get po -A --sort-by="{.metadata.creationTimestamp}"'|tee /opt/course/5/find_pods.sh

echo 'kubectl get po -A --sort-by="{.metadata.uid}"'|tee /opt/course/5/find_pods_uid.sh
```
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: safari-pv
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/Volumes/Data"
```
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: safari-pvc
  namespace: project-tiger
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  volumeName: safari-pv
```
```
kubectl create deploy safari -n project-tiger --image httpd:2.4.41-alpine -oyaml --dry-run=client|tee 06c.yaml
```
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: safari
  namespace: project-tiger
spec:
  replicas: 1
  selector:
    matchLabels:
      app: safari
  template:
    metadata:
      labels:
        app: safari
    spec:
      containers:
      - image: httpd:2.4.41-alpine
        name: httpd
        volumeMounts:
        - mountPath: "/tmp/safari-data"
          name: task-pv-storage
      volumes:
      - name: task-pv-storage
        persistentVolumeClaim:
          claimName: safari-pvc
```
